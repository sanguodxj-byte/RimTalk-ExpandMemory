# 向量服务更新报告：均值池化与模型升级

## 1. 问题诊断

之前的向量相似度计算存在严重缺陷，导致不相关的文本（如"啊哈哈哈哈"）与查询（"黄金色的树"）产生异常高的相似度。

**根本原因**：
1. **错误的向量提取**：代码仅提取了模型输出的第一个 token 的前 384 维向量。
   - BERT 模型输出维度为 `[Batch, Sequence, Hidden]` (例如 `[1, 128, 768]`)。
   - 仅取第一个 token 忽略了整个句子的语义信息。
   - 仅取前 384 维截断了向量特征（text2vec-base-chinese 是 768 维）。
2. **模型不匹配**：使用了通用的 BERT 模型，而不是针对中文优化的 `text2vec-base-chinese`。

## 2. 实施的解决方案

### 2.1 模型升级
- **旧模型**: `bert_model.onnx`
- **新模型**: `text2vec-base-chinese.onnx`
- **优势**: 专门针对中文语义匹配优化，能更好地理解中文语境和语义关系。

### 2.2 均值池化 (Mean Pooling)
实现了标准的 BERT 句子向量提取算法：

```csharp
private float[] MeanPooling(float[] outputData, long[] inputIds)
{
    // 1. 遍历所有 token
    // 2. 跳过 PAD token
    // 3. 对所有有效 token 的向量进行累加
    // 4. 除以有效 token 数量（求平均）
    // 5. 进行 L2 归一化
}
```

**原理**：
通过综合考虑句子中每个字的语义向量，生成一个代表整句含义的稠密向量。这比仅使用 CLS token 或首个 token 准确得多。

### 2.3 维度修正
- **旧维度**: 384
- **新维度**: 768
- **影响**: 完整保留了模型的语义特征空间。

## 3. 预期效果

1. **语义准确性大幅提升**：
   - "黄金色的树" 将与 "黄金树是一颗黄金色的树" 高度匹配。
   - 无意义文本（如"啊哈哈哈哈"）的相似度将显著降低。

2. **鲁棒性增强**：
   - 对文本长度不敏感（通过平均化处理）。
   - 能更好地处理同义词和近义词。

## 4. 后续建议

1. **重建向量库**：由于向量维度和模型都发生了变化，必须重新生成所有现有知识条目的向量。
2. **监控日志**：观察 `[DEBUG]` 日志中的 `First 5 pooled & normalized values`，确保数值分布合理（通常在 -0.1 到 0.1 之间）。
